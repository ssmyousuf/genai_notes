- Generative AI relies on sophisticated [machine learning](https://www.ibm.com/think/topics/machine-learning) models called _[deep learning](https://www.ibm.com/think/topics/deep-learning)_ _models_ algorithms that simulate the learning and decision-making processes of the human brain. 
- These models work by identifying and encoding the patterns and relationships in huge amounts of data, and then using that information to understand users' natural language requests or questions and respond with relevant new content.
## How generative AI works

For the most part, generative AI operates in three phases: 

- **Training**, to create a foundation model that can serve as the basis of multiple gen AI applications.  
- **Tuning**, to tailor the foundation model to a specific gen AI application.  
- **Generation**, **evaluation and retuning**_,_ to assess the gen AI application's output and continually improve its quality and accuracy.
- 
Training
- **Foundation Model Definition**: A large deep learning model trained on extensive, unstructured, unlabeled data. Acts as the base for generative AI applications across text, images, video, sound, and multimodal content.
    
- **Training Data**: Uses massive datasets (terabytes of diverse internet data) to learn broad patterns, relationships, and entities.
    
- **Self-supervised Training**: Performs 'fill in the blank' prediction exercises (e.g., next word in a sentence) and constantly minimizes prediction errors to improve understanding.
    
- **Neural Networks**: Encodes knowledge in parameters representing learned representations and relationships, enabling autonomous content generation in response to prompts.
    
- **High Computational Cost**: Training requires thousands of GPUs, weeks of processing, and can cost millions of dollars.
    
- **Open-Source Models**: Projects like Meta's Llama-2 provide pre-trained foundation models, allowing AI developers to avoid the cost and complexity of training from scratch.
    
- **Adaptability**: Foundation models can be further fine-tuned for specific tasks, making them highly versatile and efficient compared to building models for each unique use-case.
    
- **Multimodal Capability**: Supports generation and understanding across text, images, video, sound, and more, often in one unified architecture.